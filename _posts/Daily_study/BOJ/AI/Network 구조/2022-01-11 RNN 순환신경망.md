---
title: "RNN(순환신경망)"
categories:
  - Daily Study
published: false
tags:
  - Al
  - Neural Network
  - RNN
---

> 내용, 그림 출처 : [Colahl님의 블로그](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
# Sequence (Model)

## Sequence Data 
- 순서가 있는 데이터를 뜻합니다. 
- 예를 들어 Text 문맥, 시계열 데이터의 시간, 영상, 음성등의 데이터가 여기에 해당합니다. 

## Sequence Model
- Sequence Data의 특징들을 추출해 학습하고 예측, 탐지등에 사용됩니다.
- RNN, GRU, LSTM이 있습니다. 
- 시계열 예측은 모델의 종류 중 many to one에 해당합니다.


# RNN (순환신경망)
- 순환 신경망이라는 말 그대로 순환하는 신경망을 뜻합니다. 

    ![image](https://user-images.githubusercontent.com/38587274/149100938-0a899a7f-ec4d-4a25-acb4-144699d322aa.png)

- 위 그림에서 알 수 있듯이 Loop를 가지는 모델입니다. 
- Output으로 나간 결과가 다시 Input으로 들어오는 구조입니다. 
- 이 모델을 시간대별로 나열해보면 아래 사진이 도출됩니다. 
    ![image](https://user-images.githubusercontent.com/38587274/149110180-431a0fb4-2182-447d-bd70-28b50eb1e6e3.png)
## 장점
-  해당 모델은 Time stamp마다 동일한 Function과 Parameter를 사용하기 때문에 Input, Output Size의 영향을 받지 않습니다. 

## 문제점
- 역전파 과정에서는 W를 구하는 과정이 Chain Rule에 의해 Layer들의 기울기를 곱하는 방식으로 이뤄지기 때문에 Layer가 깊어질수록  입력층의 기울기가 점차적으로 작아져 소실되는 Vanishing Gradient현상이 발생할 수 있습니다.
- > ex ) 0.9 * 0.9 * 0.9 * .... *0.9(100번) --> 아주 작은 수 
-  또한 반대로 기울기가 발산하는 Exploding Gradient 현상이 발생할 수 있습니다.
- > ex ) 1.2 * 1.2 * 1.2 * .... *1.2(100번) --> 아주 큰 수 
- 문제가 되는 이유는 입력정보와 입력정보를 사용하는 출력지점의 거리가 멀 경우 기울기가 점점 작아지거나 커져 학습능력이 크게 저하되기 때문입니다. 
- 기울기를 약간만 키워도 너무 커지고 약간만 줄여도 거의 소실되어 설정하기도 어렵습니다. 

## 해결방법
- 이 모델은 사이사이에 기울기를 조절하는 Gate를 설치한 모델인 LSTM을 사용하는 것입니다.

# LSTM(Long Short-Term Memory)

![image](https://user-images.githubusercontent.com/38587274/149119762-cf8a314d-5114-49f3-9263-0bf010cf37e7.png)

- 위 모델은 단순히 Activation을 하는게 아니라 각 시간 마다 가중치를 적절하게 조절하는 Gate를 각 Unit마다 설치해 가중치를 조절합니다. 

- 간단한 예시
    - "나는 <span style = "color:red"> 한국</span>사람이야. 그래서 나는 ~ 하고 ~ 해서 ~ ....(생략) <span style = "color:orange"> 한국어<span style = "color:lightblue">(예측할 단어)</span></span>를 잘해.
    
> 위의 예시에서 한국어라는 Key를 기억해야지 '한국어'를 예측할 수 있다. 하지만 RNN은 오래전 정보는 기억을 하지 못하기 때문에 예측하기 힘들다. 반대로 Long term Dependencies를 가지고 있는 LSTM은 해당 정보를 오래동안 기억할 수 있어 RNN에 비해 예측할 확률이 높다. !!